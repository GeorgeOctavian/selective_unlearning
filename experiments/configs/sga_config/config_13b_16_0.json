{
    "mode": "unlearn",
    "privacy_method": "ufl",
    "wandb_run_name": "sga",
    "wandb_project": "icml_results",
    "wandb_log": true,
    "peft": false,
    "num_train_epochs": 150,
    "num_train_epochs_nb": 2,
    "lambda_weight": 0.3,
    "check_val_every_n_epoch": 1,
    "check_validation_only": false,
    "do_init_eval": true,
    "train_set": "experiments/data/extraction/lm_extraction_16_0.csv",
    "valid_sets": [
        "experiments/data/extraction/lm_extraction_16_0.csv",
        "experiments/data/validation_data/lambada.csv",
        "piqa",
        "hellaswag",
        "ai2_arc",
        "ai2_arc",
        "super_glue",
        "winogrande",
        "math_qa",
        "experiments/data/validation_data/pubmed_qa.csv",
        "experiments/data/validation_data/wizard_of_wikipedia.json",
        "experiments/data/validation_data/empathetic_dialogues.json",
        "experiments/data/validation_data/blended_skill_talk.json",
        "experiments/data/validation_data/wizard_of_internet.json"
    ],
    "valid_subset_path": [
        "",
        "",
        "",
        "",
        "ARC-Easy",
        "ARC-Challenge",
        "copa",
        "winogrande_s",
        "",
        "",
        "",
        "",
        "",
        ""
    ],
    "valid_type_path": [
        "target",
        "test",
        "validation",
        "validation",
        "validation",
        "validation",
        "validation",
        "validation",
        "validation",
        "",
        "",
        "",
        "",
        ""
    ],

    "first_coef": -3.0,
    "second_coef": 0,

    "train_batch_size": 8,
    "eval_batch_size": 8,
    "gradient_accumulation_steps": 1,
    "gpu_list": [
        2,3
    ],

    "nb_epoch": "00",
    "mask_pct": 0.7,
    "n_perturbations": 10,
    "mask_model": "t5-large",

    "learning_rate": 5e-5,
    "model_name_or_path": "EleutherAI/gpt-neo-2.7b",
    "el_threshold": 0.0499,

    "mem_threshold": 0.70,
    "first_threshold": 0.9,
    "ma_threshold": 0.33,

    "input_length": 512,
    "output_length": 512,
    "target_length": 200,
    "num_workers": 8,
    "strategy": "deepspeed_stage_2_offload",
    "fp16": true
}
